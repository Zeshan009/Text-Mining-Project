{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "#Import Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word=word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    stemmer = porter_stemmer()\n",
    "    return stemmer.stem(word=word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre():\n",
    "    # Reading training and test files to list data structures\n",
    "    data = pd.read_csv(\"Sentiment Analysis Dataset.csv\", sep = \"\\t\", index_col=False, encoding='latin-1',low_memory=False)\n",
    "    df = DataFrame(data)\n",
    "#     print(df['Sentiment'])\n",
    "    labelCount = df.groupby(df['Sentiment']).count()\n",
    "    print(labelCount)\n",
    "    x = df['SentimentText'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "    y = df['Sentiment']\n",
    "#     .astype(str)\n",
    "    x = x.str.replace('[^a-zA-Z0-9-_*.]', ' ')\n",
    "    x_check = [\" \".join([lemmatize(word) for word in sentence.split(\" \")]) for sentence in x]\n",
    "    #temp_df = [filter(lambda i: i not in string.punctuation,sentence) for sentence in x_check]\n",
    "    return x_check, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelEncoding(y):\n",
    "    labelEncoder = LabelEncoder()\n",
    "    y_encoded = labelEncoder.fit_transform(y)\n",
    "    y_encoded\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countVectorizer(x):\n",
    "    stopset = set(stopwords.words('English'))\n",
    "    vect = CountVectorizer(analyzer='word', encoding='utf-8', min_df = 0, ngram_range=(1, 1), lowercase = True, strip_accents='ascii', stop_words = stopset)\n",
    "    X_vec = vect.fit_transform(x)\n",
    "    return X_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfVectorizer(x):\n",
    "    stopset = set(stopwords.words('English'))\n",
    "    vect = TfidfVectorizer(analyzer='word', encoding='utf-8', min_df = 0, ngram_range=(1, 1), lowercase = True, strip_accents='ascii', stop_words = stopset)\n",
    "    X_vec = vect.fit_transform(x)\n",
    "    return X_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTestTrain(X_vec, y_encoded):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vec, y_encoded, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.5, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPreRec(naiveBayesRecall, naiveBayesPrecision, svmRecall, svmPrecision, randomForestRecall, randomForestPrecision, logisticRegressionRecall, logisticRegressionPrecision, sgdRecall, sgdPrecision):    \n",
    "    plt.plot([naiveBayesRecall],[naiveBayesPrecision], 'ro')\n",
    "    plt.plot([svmRecall],[svmPrecision], 'ms')\n",
    "    plt.plot([randomForestRecall],[randomForestPrecision], 'yo')\n",
    "    plt.plot([logisticRegressionRecall],[logisticRegressionPrecision], 'go')\n",
    "    plt.plot([sgdRecall],[sgdPrecision], 'xb-')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall comparison plot')\n",
    "    plt.legend(['MNB', 'SVM', 'RF', 'LR', 'SGD'], loc='upper left')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAcuuracyComaprisonGraph(naiveBayesFMeasure, svmFMeasure, randomForestFMeasure, logisticRegressionFMeasure, sgdFMeasure):\n",
    "#Accuracy Comparison Plot\n",
    "    cl = ('MNB', 'SVC', 'RF', 'LR', 'SGD')\n",
    "    y_pos = np.arange(len(cl))\n",
    "    acc = [77.2682926829,79.0243902439,76.8780487805,80.6829268293,75.3170731707]\n",
    "    plt.bar(y_pos, acc, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, cl)\n",
    "    plt.title('Accuracy Comparison Plot')\n",
    "    plt.show()\n",
    "    cl = ('MNB', 'SVC', 'RF', 'LR', 'SGD')\n",
    "    y_pos = np.arange(len(cl))\n",
    "    acc = [naiveBayesFMeasure, svmFMeasure, randomForestFMeasure, logisticRegressionFMeasure, sgdFMeasure]\n",
    "    plt.bar(y_pos, acc, align='center', alpha=1.0)\n",
    "    plt.xticks(y_pos, cl)\n",
    "    plt.title('F Measure Comparison Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyNaiveBayesClassifier(X_train, y_train, X_test, y_test):\n",
    "    # Thanks to sklearn, let us quickly train some multinomial models\n",
    "    # Model Training: Multinomial Naive Bayes\n",
    "        mnb_classifier = MultinomialNB()\n",
    "        mnb_classifier.fit(X_train, y_train)\n",
    "        model_accuracies = cross_val_score(estimator=mnb_classifier, \n",
    "                                       X=X_train, y=y_train, cv=10)\n",
    "        print(\"Model Accuracies Mean\", model_accuracies.mean()*100)\n",
    "        print(\"Model Accuracies Standard Devision\", model_accuracies.std()*100)\n",
    "    # Model Testing: Multinomial Naive Bayes\n",
    "        y_pred = mnb_classifier.predict(X_test)\n",
    "        metrics.confusion_matrix(y_test, y_pred)\n",
    "        test_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "        precision_mnb = precision_score(y_test, y_pred, average='macro')  \n",
    "        recall_mnb = recall_score(y_test, y_pred, average='macro') \n",
    "        f_mnb = 2*(precision_mnb*recall_mnb)/(precision_mnb+recall_mnb)\n",
    "        print(\"Multinomial Naive Bayes Classifier Test Accuracy: \", test_accuracy*100)\n",
    "        print(\"Multinomial Naive Bayes Classifier Test Precision: \", precision_mnb*100)\n",
    "        print(\"Multinomial Naive Bayes Classifier Test Recall: \", recall_mnb*100)\n",
    "        print(\"Multinomial Naive Bayes Classifier Test F measure: \", f_mnb*100)\n",
    "        return precision_mnb, recall_mnb, f_mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applySVMClassifier(X_train, y_train, X_test, y_test):\n",
    "    # Model Training: SVMs\n",
    "    svc_classifier = SVC(kernel='linear', random_state=0)\n",
    "    svc_classifier.fit(X_train, y_train)\n",
    "    model_accuracies = cross_val_score(estimator=svc_classifier, \n",
    "                                   X=X_train, y=y_train, cv=10) \n",
    "    print(\"Model Accuracies Mean\", model_accuracies.mean()*100)\n",
    "    print(\"Model Accuracies Standard Devision\", model_accuracies.std()*100)\n",
    "    # Model Testing: SVMs\n",
    "    y_pred = svc_classifier.predict(X_test)\n",
    "    metrics.confusion_matrix(y_test, y_pred)\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision_SVC = precision_score(y_test, y_pred, average='macro')  \n",
    "    recall_SVC = recall_score(y_test, y_pred, average='macro') \n",
    "    f_SVC = 2*(precision_SVC * recall_SVC) / (precision_SVC + recall_SVC)\n",
    "    print(\"SVCs Test Accuracy: \", test_accuracy*100)\n",
    "    print(\"SVCs Test Precision: \", precision_SVC*100)\n",
    "    print(\"SVCs Test Recall: \", recall_SVC*100)\n",
    "    print(\"SVCs Test F measure: \", f_SVC*100)\n",
    "    return precision_SVC, recall_SVC, f_SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyRandomForestClassifier(X_train, y_train, X_test, y_test):\n",
    "    # Model Training: Random Forests Classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\",\n",
    "                                        criterion='entropy', random_state=1)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    model_accuracies = cross_val_score(estimator=rf_classifier, \n",
    "                                   X=X_train, y=y_train, cv=5) \n",
    "    print(\"Model Accuracies Mean\", model_accuracies.mean()*100)\n",
    "    print(\"Model Accuracies Standard Devision\", model_accuracies.std()*100)\n",
    "    # Model Testing: Random Forests Classifier\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "    metrics.confusion_matrix(y_test, y_pred)\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision_RF = precision_score(y_test, y_pred, average='macro')  \n",
    "    recall_RF = recall_score(y_test, y_pred, average='macro') \n",
    "    f_RF = 2*(precision_RF * recall_RF) / (precision_RF + recall_RF)\n",
    "    print(\"Random Forests Test Accuracy: \", test_accuracy*100)\n",
    "    print(\"Random Forests Test Precision: \", precision_RF*100)\n",
    "    print(\"Random Forests Test Recall: \", recall_RF*100)\n",
    "    print(\"Random Forests Test F measure: \", f_RF*100)\n",
    "    return precision_RF, recall_RF, f_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyLogisticRegressionClassifier(X_train, y_train, X_test, y_test):\n",
    "    #Apply Logistic Regression Classifier\n",
    "    lr = LogisticRegression(penalty = 'l2', C = 1)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    metrics.confusion_matrix(y_test, y_pred)\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision_LR = precision_score(y_test, y_pred, average='macro')  \n",
    "    recall_LR = recall_score(y_test, y_pred, average='macro') \n",
    "    f_LR = 2*(precision_LR * recall_LR) / (precision_LR + recall_LR)\n",
    "    print(\"LogisticRegression_classifier Accuracy percent:\",test_accuracy *100)\n",
    "    print(\"LogisticRegression_classifier Precision percent:\",precision_LR *100)\n",
    "    print(\"LogisticRegression_classifier Recall percent:\",recall_LR *100)\n",
    "    print(\"LogisticRegression_classifier F measure:\",f_LR *100)\n",
    "    return precision_LR, recall_LR, f_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applySGDClassifier(X_train, y_train, X_test, y_test):\n",
    "    #Apply SGD Classifier\n",
    "    SGDClassifier_classifier = SGDClassifier()\n",
    "    SGDClassifier_classifier.fit(X_train, y_train)\n",
    "    y_pred = SGDClassifier_classifier.predict(X_test)\n",
    "    metrics.confusion_matrix(y_test, y_pred)\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision_SGD = precision_score(y_test, y_pred, average='macro')  \n",
    "    recall_SGD = recall_score(y_test, y_pred, average='macro')\n",
    "    f_SGD = 2*(precision_SGD * recall_SGD) / (precision_SGD + recall_SGD)\n",
    "    print(\"SGD_classifier Accuracy percent:\",test_accuracy *100)\n",
    "    print(\"SGD_classifier Precision percent:\",precision_SGD *100)\n",
    "    print(\"SGD_classifier Recall percent:\",recall_SGD *100)\n",
    "    print(\"SGD_classifier Recall F measure:\",f_SGD *100)\n",
    "    return precision_SGD, recall_SGD, f_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDecisionTreeClassifier(X_train, y_train, X_test, y_test):\n",
    "    #Apply Decision Tree Classifier\n",
    "    Decision_Tree_CLF = DecisionTreeClassifier(random_state=0)\n",
    "    Decision_Tree_CLF.fit(X_train, y_train)\n",
    "    y_pred = Decision_Tree_CLF.predict(X_test)\n",
    "    metrics.confusion_matrix(y_test, y_pred)\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision_DT = precision_score(y_test, y_pred, average='macro')  \n",
    "    recall_DT = recall_score(y_test, y_pred, average='macro')\n",
    "    f_DT = 2*(precision_DT * recall_DT) / (precision_DT + recall_DT)\n",
    "    print(\"SGD_classifier Accuracy percent:\",test_accuracy *100)\n",
    "    print(\"SGD_classifier Precision percent:\",precision_DT *100)\n",
    "    print(\"SGD_classifier Recall percent:\",recall_DT *100)\n",
    "    print(\"SGD_classifier Recall F measure:\",f_DT *100)\n",
    "    return precision_DT, recall_DT, f_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolveOverSampling(X_train, y_train, X_test, y_test):\n",
    "    # Over Sampling\n",
    "    sm = SMOTE(random_state=0, ratio = 1.0)\n",
    "    x_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "    x_test_res, y_test_res = sm.fit_sample(X_test, y_test)\n",
    "    \n",
    "def resolveUnderSampling(X_train, y_train, X_test, y_test):\n",
    "    # Under Sampling\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    x_train_res, y_train_res= rus.fit_sample(X_train, y_train)\n",
    "    x_test_res, y_test_res = rus.fit_sample(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLabels(y):\n",
    "    #Encoding y\n",
    "    y_encoded = labelEncoding(y)\n",
    "    #Count Labels and plot them\n",
    "    y_count = Counter(y_encoded)\n",
    "    key = y_count.keys()\n",
    "    df = pd.DataFrame(y_count,index=key)\n",
    "    df.drop(df.columns[1:], inplace=True)\n",
    "    df.plot(kind='bar')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
